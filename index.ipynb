{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactions - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll explore interactions in the Boston Housing data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to:\n",
    "- Understand what interactions are\n",
    "- Understand how to accommodate for interactions in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a baseline model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use a couple of built-in functions, which we imported for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Boston data set using `load_boston()`. We won't bother to preprocess the data in this lab. If you still want to build a model in the end, you can do that, but this lab will just focus on finding meaningful insights in interactions and how they can improve $R^2$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a baseline model which includes all the variables in the Boston housing data set to predict the house prices. The use 10-fold cross-validation and report the mean $R^2$ value as the baseline $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "y = pd.DataFrame(boston['target'], columns = ['target'])\n",
    "full = pd.concat([y,X], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7190106820189477"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "baseline = np.mean(cross_val_score(regression, X, y, scoring=\"r2\", cv=crossvalidation))\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how interactions improve your baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create all possible combinations of interactions, loop over them and add them to the baseline model one by one to see how they affect the R^2. We'll look at the 3 interactions which have the biggest effect on our R^2, so print out the top 3 combinations.\n",
    "\n",
    "You will create a for loop to loop through all the combinations of 2 predictors. You can use `combinations` from itertools to create a list of all the pairwise combinations. To find more info on how this is done, have a look [here](https://docs.python.org/2/library/itertools.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "combinations = list(combinations(boston.feature_names, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CRIM', 'CHAS', 0.7224266827041401], ['CRIM', 'NOX', 0.720956844677177], ['CRIM', 'RAD', 0.7218426607693017], ['CRIM', 'TAX', 0.7211937125038534], ['CRIM', 'PTRATIO', 0.7203531457728599], ['CRIM', 'LSTAT', 0.7204731402012098], ['ZN', 'INDUS', 0.7245130249509673], ['ZN', 'RM', 0.7288206411527088], ['ZN', 'DIS', 0.7198623421930656], ['INDUS', 'RM', 0.757780514352017], ['INDUS', 'AGE', 0.7222863283271979], ['INDUS', 'DIS', 0.7283635108497103], ['INDUS', 'TAX', 0.7223792494260637], ['INDUS', 'LSTAT', 0.7193723158986447], ['CHAS', 'RAD', 0.7199062643047318], ['CHAS', 'TAX', 0.7204452777744265], ['CHAS', 'B', 0.7196376401298062], ['CHAS', 'LSTAT', 0.7210130989247118], ['NOX', 'RM', 0.7453599495065298], ['NOX', 'DIS', 0.7195305930076332], ['NOX', 'RAD', 0.7216402620446947], ['NOX', 'TAX', 0.7194683252011806], ['NOX', 'PTRATIO', 0.7208556256614022], ['NOX', 'B', 0.7192004212855704], ['RM', 'AGE', 0.740659876089026], ['RM', 'DIS', 0.7342130965590978], ['RM', 'RAD', 0.7682152400234057], ['RM', 'TAX', 0.7750525123747647], ['RM', 'PTRATIO', 0.7633759067582868], ['RM', 'B', 0.738566570708287], ['RM', 'LSTAT', 0.7864889421124028], ['AGE', 'DIS', 0.7210193945912321], ['AGE', 'PTRATIO', 0.719524763509534], ['AGE', 'B', 0.7209442240026319], ['DIS', 'PTRATIO', 0.7212530527937469], ['DIS', 'B', 0.7191799627871103], ['TAX', 'PTRATIO', 0.7220753714337282]]\n"
     ]
    }
   ],
   "source": [
    "## code to find top 3 interactions by R^2 value here\n",
    "R2 = []\n",
    "X_interact = X.copy()\n",
    "for comb in combinations:\n",
    "    X_interact['interaction'] = X_interact[comb[0]] * X_interact[comb[1]]\n",
    "    interact_R2 = np.mean(cross_val_score(regression, X_interact, y, scoring=\"r2\", cv=crossvalidation))\n",
    "    R2.append([comb[0], comb[1], interact_R2])\n",
    "print(R2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7864889421124028, 0.7750525123747647, 0.7682152400234057]\n",
      "RM RAD\n",
      "RM TAX\n",
      "RM LSTAT\n"
     ]
    }
   ],
   "source": [
    "R_list = []\n",
    "for i in range(len(R2)):\n",
    "    R_list.append(R2[i][2])\n",
    "R_list = sorted(R_list, reverse = True)\n",
    "top_3 = R_list[:3]\n",
    "print(top_3)\n",
    "for i in range(len(R2)):\n",
    "    for item in top_3:\n",
    "        if R2[i][2] == item:\n",
    "            print(R2[i][0], R2[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the top 3 interactions: \"RM\" as a confounding factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three interactions seem to involve \"RM\", the number of rooms as a confounding variable for all of them. Let's have a look at interaction plots for all three of them. This exercise will involve:\n",
    "\n",
    "- splitting our data up in 3 groups: one for houses with a few rooms, one for houses with a \"medium\" amount of rooms, one for a high amount of rooms.\n",
    "- Create a function `build_interaction_rm`. This function takes an argument `varname` (which can be set equal to the column name as a string) and a column `description` (which describes the variable or varname, to be included on the x-axis of the plot). The function outputs a plot that uses \"RM\" as a confounding factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set for high, medium and low amount of rooms for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = np.asarray(X[[\"RM\"]]).reshape(len(X[[\"RM\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_rm = full[rm > np.percentile(rm, 67)]\n",
    "med_rm = full[(rm > np.percentile(rm, 33)) & (rm <= np.percentile(rm, 67))]\n",
    "low_rm = full[rm <= np.percentile(rm, 33)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `build_interaction_rm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interaction_rm(varname, description):\n",
    "    reg_high = LinearRegression()\n",
    "    reg_med = LinearRegression()\n",
    "    reg_low = LinearRegression()\n",
    "    \n",
    "    rm_high = high_rm[varname].values.reshape(-1, 1)\n",
    "    rm_med = med_rm[varname].values.reshape(-1,1)\n",
    "    rm_low = low_rm[varname].values.reshape(-1,1)\n",
    "    \n",
    "    pred_high = reg_high.fit(rm_high, reg_high['target'])\n",
    "    pred_men "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use build_interaction_rm with the three variables that came out with the highest effect on $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a final model including all three interactions at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $R^2$ has increased considerably! Let's have a look in statsmodels to see if all these interactions are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your conclusion here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formulate your conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now understand how to include interaction effects in your model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
